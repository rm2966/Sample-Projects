{
 "cells": [
  {
   "cell_type": "raw",
   "id": "56b4f25c-8995-497a-98ed-46ae1e66e599",
   "metadata": {},
   "source": [
    "Summary for the Intro \n",
    "\n",
    "This code combines the use of FastAPI, Hugging Face Transformers, TF-IDF vectorization, and cosine similarity to build a basic retrieval-augmented generation (RAG) model that provides liquidity risk assessments for Company X.\n",
    "\n",
    "FastAPI is used to build a web API that can serve the results of the risk assessment on an HTTP endpoint.\n",
    "Hugging Face's GPT-2 model is integrated to generate natural language responses based on relevant financial documents retrieved from a simulated database.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity are used to rank and retrieve the most relevant documents based on a user query.\n",
    "Corrective RAG is applied in case the initial GPT-2 response does not sufficiently answer the userâ€™s query.\n",
    "The code includes preprocessing steps, document ranking, GPT-2 language generation, and a corrective mechanism to ensure that responses address the query appropriately.\n",
    "\n",
    "Libraries Used:\n",
    "FastAPI: Used to create the API that serves the liquidity risk assessment.\n",
    "transformers: Hugging Face library that loads pre-trained models (GPT-2 in this case) and tokenizers.\n",
    "TfidfVectorizer (from sklearn.feature_extraction.text): Converts documents into vectors for numerical comparisons using term frequency-inverse document frequency.\n",
    "cosine_similarity (from sklearn.metrics.pairwise): Measures the similarity between the query and documents based on the TF-IDF vectors.\n",
    "numpy: Provides support for array operations and is used for ranking documents based on cosine similarity.\n",
    "requests: Python library used for making HTTP requests to the FastAPI server from within the Jupyter notebook.\n",
    "Why These Libraries Were Chosen:\n",
    "FastAPI: To easily deploy a RESTful API that can be queried from any HTTP client, allowing for easy interaction with the GPT-2 model and document retrieval pipeline.\n",
    "transformers (GPT-2): Hugging Face provides pre-trained language models that generate natural language text. GPT-2 is used here to generate responses based on the financial documents retrieved.\n",
    "TfidfVectorizer: This is a standard method for converting text data into numerical vectors that can be used for similarity measurements. It helps identify the most relevant documents by comparing the query with the documents' contents.\n",
    "cosine_similarity: A widely used method to measure the similarity between two vectors (in this case, between the query vector and document vectors). It's simple, efficient, and commonly used in information retrieval systems.\n",
    "requests: Used to query the FastAPI server from within the Jupyter notebook, making it easy to integrate API responses into notebook workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7286c6a-130e-46cf-a281-5a1231247e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from fastapi import FastAPI  # API framework to handle HTTP requests and serve APIs\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer  # Hugging Face transformers to load GPT-2 model and tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF for transforming text into numerical vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Cosine similarity for ranking documents based on query\n",
    "import numpy as np  # Numpy for array operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1aec5f-0e79-4161-b47f-9c3aed60aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Simulated database of financial documents with metadata tagging for retrieval\n",
    "database = {\n",
    "    \"doc1\": {\"text\": \"Company X has significant liquidity risk due to its high debt ratio.\",\n",
    "             \"metadata\": [\"liquidity risk\", \"debt\"]},\n",
    "    \"doc2\": {\"text\": \"Company X maintains a solid cash reserve, which mitigates some of the liquidity concerns.\",\n",
    "             \"metadata\": [\"cash reserve\", \"liquidity\"]},\n",
    "    \"doc3\": {\"text\": \"The current market conditions have led to increased liquidity stress for many firms, including Company X.\",\n",
    "             \"metadata\": [\"market conditions\", \"liquidity stress\"]},\n",
    "    \"doc4\": {\"text\": \"Company X has recently restructured its debt, which lowers its short-term liquidity risk.\",\n",
    "             \"metadata\": [\"debt restructuring\", \"liquidity risk\"]}\n",
    "}\n",
    "\n",
    "# Step 1: User Query\n",
    "user_query = \"What is the liquidity risk for Company X?\"\n",
    "\n",
    "# Step 2: Data Retrieval with Metadata Tagging\n",
    "def retrieve_documents(query, database):\n",
    "    relevant_docs = []\n",
    "    for doc_id, doc_data in database.items():\n",
    "        text = doc_data['text']\n",
    "        metadata = doc_data['metadata']\n",
    "        if any(tag in query.lower() for tag in metadata):\n",
    "            relevant_docs.append(text)\n",
    "    return relevant_docs\n",
    "\n",
    "# Retrieve relevant documents based on the user's query\n",
    "documents = retrieve_documents(user_query, database)\n",
    "\n",
    "# Step 3: Preprocessing (Tokenization and Lowercasing)\n",
    "def preprocess_text(text):\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Preprocess all retrieved documents\n",
    "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Step 4: Indexing (TF-IDF for Vectorization)\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 5: Preprocessing the user query for similarity comparison\n",
    "query_vector = vectorizer.transform([user_query])\n",
    "\n",
    "# Step 6: Ranking with Metadata (Cosine Similarity)\n",
    "similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "ranked_doc_indices = np.argsort(similarities)[::-1]\n",
    "ranked_docs = [documents[i] for i in ranked_doc_indices]\n",
    "\n",
    "# Step 7: LLM Integration (GPT-2 for language generation)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Assign the eos_token to the pad_token to avoid padding errors\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate input context for the model using the top 2 ranked documents\n",
    "context = \" \".join(ranked_docs[:2])\n",
    "input_ids = tokenizer.encode(context, return_tensors='pt', padding=True)\n",
    "\n",
    "# Set pad_token_id to eos_token_id to handle padding correctly\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create an attention mask to avoid confusion between padding and real tokens\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate the response using GPT-2\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated output back to text, setting clean_up_tokenization_spaces to avoid the warning\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Step 8: Corrective RAG (If Needed)\n",
    "def corrective_rag(generated_text, database):\n",
    "    if \"liquidity risk\" not in generated_text:\n",
    "        additional_docs = retrieve_documents(\"liquidity risk\", database)\n",
    "        additional_context = \" \".join(additional_docs[:2])\n",
    "        input_ids = tokenizer.encode(additional_context, return_tensors='pt')\n",
    "        output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return generated_text\n",
    "\n",
    "# Apply corrective RAG if the generated response is incomplete\n",
    "final_response = corrective_rag(generated_text, database)\n",
    "\n",
    "# Step 9: Output the Final Risk Assessment (Handled by FastAPI)\n",
    "@app.get(\"/assess_liquidity_risk\")\n",
    "def assess_liquidity_risk():\n",
    "    return {\"query\": user_query, \"response\": final_response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2a88ef-aab8-456a-9a53-938080af48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the liquidity risk for Company X?', 'response': \"Company X maintains a solid cash reserve, which mitigates some of the liquidity concerns. Company X has significant liquidity risk due to its high debt ratio.\\n\\nThe Company's cash and cash equivalents are limited to $1.5 billion and $1.6 billion, respectively.\\n\\nThe Company's net income is $1.1 billion and $1.2 billion, respectively.\\n\\nThe Company's net income is $1.3 billion and $1.4 billion, respectively\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Make a GET request to your FastAPI endpoint\n",
    "response = requests.get(\"http://127.0.0.1:8000/assess_liquidity_risk\")\n",
    "\n",
    "# Print the JSON response from the FastAPI server\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec747b-fc85-44d7-a906-eb7454af1b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
