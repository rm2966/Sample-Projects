{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48d38d9",
   "metadata": {},
   "source": [
    "Updated Summary of Code Functionality:\n",
    "\n",
    "This code leverages advanced natural language processing (NLP) techniques and AI-driven strategies to generate personalized educational recommendations based on a combination of student performance data, textbook content, and metadata tagging. It integrates several state-of-the-art AI tools and libraries, such as AWS Bedrock for large-scale model deployment, Hugging Face Transformers for text tokenization and preprocessing, NLTK for foundational NLP tasks, BM25 for document ranking, and Sentence Transformers for semantic similarity analysis. The script employs Large Language Models (LLMs) to enhance the generation of educational content, and incorporates metadata tagging, predictive modeling, and learner engagement analysis to provide a more refined and targeted educational experience. Key functionalities include:\n",
    "\n",
    "Text Preprocessing: The code preprocesses input text using tokenization, stemming, and stop-word removal techniques to clean and standardize the data, ensuring it is suitable for analysis and modeling.\n",
    "\n",
    "Metadata Tagging: Each piece of textbook content is enriched with metadata tags (e.g., \"Introduction,\" \"Advanced Techniques,\" \"Machine Learning\") that provide additional context and categorization. This tagging enables more accurate retrieval of relevant content based on the student's learning needs and enhances the personalization of recommendations.\n",
    "\n",
    "Document Ranking with BM25: The BM25 ranking function is used to prioritize textbook chapters or sections based on their relevance to queries derived from each student's weak areas. Metadata tags further refine this ranking by adding contextual weight, helping identify the most pertinent content for each student’s learning needs.\n",
    "Model Invocation on AWS Bedrock: The script utilizes a large language model hosted on AWS Bedrock to generate detailed explanations, examples, and personalized recommendations. This approach leverages AWS's powerful cloud infrastructure to deliver sophisticated natural language understanding and dynamic content generation.\n",
    "Semantic Similarity Computation: Using Sentence Transformers, the code computes semantic similarity between AI-generated text and reference texts, ensuring that the content aligns with high-quality educational standards. This step verifies that the recommendations are not only contextually accurate but also relevant to the learning objectives.\n",
    "\n",
    "Predictive Modeling and Learner Engagement Analysis: The code integrates predictive analytics to assess students' likelihood of success or difficulty in specific areas based on their performance and engagement data (e.g., time spent on content, interaction frequency). This predictive capability enables proactive adjustments to the learning path, enhancing the educational experience.\n",
    "Performance Evaluation: The code evaluates the generated outputs using a comprehensive set of metrics, including BLEU score, Exact Match, Semantic Similarity, and relevance metrics such as Precision, Recall, F1-Score, nDCG, and MAP. These metrics provide a holistic understanding of the quality, relevance, and effectiveness of the recommendations.\n",
    "\n",
    "Detailed Description of Outputs\n",
    "Personalized Learning Recommendations:\n",
    "The script generates tailored learning recommendations for each student by analyzing their performance data to identify weaker areas and leveraging metadata to refine content selection. It formulates targeted queries that focus on these areas and searches the enriched textbook content for the most relevant material to enhance the student’s understanding. This personalized strategy ensures the guidance is specifically adapted to each student's unique educational needs, optimizing their learning experience.\n",
    "\n",
    "Evaluation Metrics:\n",
    "BLEU Score: Measures the quality of the generated text by evaluating the n-gram overlap with reference texts. A higher BLEU score indicates closer lexical similarity to the reference, suggesting more accurate language use.\n",
    "Exact Match: Provides a binary score indicating whether the generated text exactly matches any reference texts, critical for tasks where precise replication is necessary.\n",
    "Semantic Similarity: Calculates cosine similarity between embeddings of the generated and reference texts using Sentence Transformers. A higher score reflects strong semantic alignment, even when the phrasing differs, ensuring the meaning is preserved.\n",
    "\n",
    "Relevance Metrics:\n",
    "Precision: The proportion of relevant documents among the retrieved documents, indicating retrieval accuracy.\n",
    "Recall: The proportion of all relevant documents that were successfully retrieved, reflecting the comprehensiveness of the retrieval process.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, providing a balanced evaluation measure.\n",
    "nDCG (Normalized Discounted Cumulative Gain): Evaluates the ranking quality of retrieved documents, taking into account the position of each relevant document.\n",
    "MAP (Mean Average Precision): Averages precision scores across all relevant documents to provide a comprehensive assessment of retrieval effectiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53eeb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rank_bm25) (1.26.4)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae35a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Learning Recommendation for Student A:\n",
      "  Deep learning plays a crucial role in NLP by enabling the development of advanced techniques such as transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT. These techniques have achieved state-of-the-art results on numerous NLP benchmarks, demonstrating the effectiveness of deep learning in NLP.\n",
      "\n",
      "The role of deep learning in NLP is to:\n",
      "\n",
      "1. Enable the development of advanced techniques: Deep learning allows for the creation of complex models that can capture subtle patterns and relationships in language data.\n",
      "2. Achieve state-of-the-art results: Deep learning models have consistently outperformed traditional NLP methods on various benchmarks, demonstrating their ability to accurately process and analyze language data.\n",
      "3. Improve language understanding: Deep learning models can learn to recognize and generate language patterns, enabling them to better understand the meaning and context of text.\n",
      "\n",
      "In summary, deep learning is a crucial component of advanced NLP techniques, enabling the development of complex models that can accurately process and analyze language data, achieve state-of-the-art results, and improve language understanding.\n",
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, nDCG: 1.00, MAP: 1.00\n",
      "BLEU Score: 0.08, Exact Match: 0, Semantic Similarity: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Personalized Learning Recommendation for Student B:\n",
      "  Deep learning plays a crucial role in NLP by enabling the development of advanced techniques such as transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT. These techniques have achieved state-of-the-art results on various NLP benchmarks, demonstrating the effectiveness of deep learning in this field.\n",
      "\n",
      "Explanation: Deep learning is a subfield of machine learning that involves the use of artificial neural networks with multiple layers to learn complex patterns in data. In NLP, deep learning has been particularly successful in tasks such as language modeling, machine translation, and text classification. The use of deep learning in NLP has enabled the development of advanced techniques such as transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT.\n",
      "\n",
      "Transformers are a type of neural network architecture that have been widely used in NLP tasks such as machine translation and text classification. They are particularly effective in handling sequential data such as text, and have been shown to outperform traditional recurrent neural network (RNN) architectures in many tasks.\n",
      "\n",
      "Attention mechanisms are a type of neural network component that allow the model to focus on specific parts of the input data. In NLP, attention mechanisms are often used to focus on specific words or phrases in a sentence, or to focus on specific parts of a document. They have been shown to be particularly effective in tasks such as machine translation and question answering.\n",
      "\n",
      "Large-scale language models like GPT-3 and BERT are pre-trained language models that have been trained on large amounts of text data. They have been shown to be highly effective in a wide range of NLP tasks, including language modeling, text classification, and question answering. These models are particularly effective because they are able to learn complex patterns in language that are difficult to capture with traditional machine learning techniques.\n",
      "\n",
      "Overall, deep learning has played a crucial role in the development of advanced NLP techniques, and has enabled the achievement of state-of-the-art results on numerous NLP benchmarks. Its ability to learn complex patterns in data has made it a powerful tool for a wide range of NLP tasks. (Word count: 276) ...more\n",
      "\n",
      "• 3. What is the role of deep learning in NLP?\n",
      "• 4. How have deep learning techniques been used in NLP?\n",
      "• 5. What are some examples of deep learning techniques used in NLP?\n",
      "\n",
      "Answer: Deep learning plays a crucial role in NLP by enabling the development of advanced techniques such as transformers, attention mechanisms, and large-scale language models like GPT-3\n",
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, nDCG: 1.00, MAP: 1.00\n",
      "BLEU Score: 0.04, Exact Match: 0, Semantic Similarity: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import boto3  # AWS SDK for Python, used to interact with AWS services like S3, Lambda, and Bedrock for model inference.\n",
    "import json  # Standard library for JSON handling, used for serializing and deserializing data in API requests and responses.\n",
    "from rank_bm25 import BM25Okapi  # BM25Okapi is a ranking function used by search engines to rank documents based on relevance.\n",
    "from transformers import AutoTokenizer  # Hugging Face's Transformers library, used for tokenizing text for NLP models.\n",
    "from nltk.stem import PorterStemmer  # NLTK's PorterStemmer, used for stemming words to their root form.\n",
    "from nltk.corpus import stopwords  # NLTK's stopwords corpus, used to filter out common stop words in English.\n",
    "import nltk  # Natural Language Toolkit, a comprehensive library for text processing and computational linguistics.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, ndcg_score, average_precision_score  # Scikit-learn metrics for evaluating the performance of retrieval and classification tasks.\n",
    "from nltk.translate.bleu_score import sentence_bleu  # BLEU score metric from NLTK, used to evaluate the quality of text generation models.\n",
    "from sentence_transformers import SentenceTransformer, util  # Sentence Transformers for semantic similarity computation using BERT-like models.\n",
    "import numpy as np  # NumPy, a fundamental package for scientific computing with Python, used here for numerical operations like argmax.\n",
    "\n",
    "# Ensure necessary NLTK data files are downloaded for stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the Bedrock client for AWS to interact with foundation models deployed on AWS Bedrock\n",
    "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Initialize SentenceTransformer model for semantic similarity\n",
    "semantic_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def invoke_model(model_id, prompt_text, input_key, output_key):\n",
    "    \"\"\"\n",
    "    Invokes a foundation model hosted on AWS Bedrock.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id (str): The identifier of the model to invoke on AWS Bedrock.\n",
    "    - prompt_text (str): The input prompt text for the model.\n",
    "    - input_key (str): The key used for the input in the API request body.\n",
    "    - output_key (str): The key to retrieve the output from the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - output (str): The generated output from the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Making an API call to AWS Bedrock model endpoint\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                input_key: prompt_text\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # Parse the response to extract the generated output\n",
    "        result = json.loads(response['body'].read().decode('utf-8'))\n",
    "        output = result.get(output_key, None)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during model invocation\n",
    "        print(f\"Error invoking model {model_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_with_huggingface(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text using Hugging Face tokenizer and NLTK.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessed_tokens (list): A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer from Hugging Face Transformers for text tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Tokenize the input text into smaller units (tokens)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Initialize NLTK's PorterStemmer for stemming words to their root form\n",
    "    stemmer = PorterStemmer()\n",
    "    # Retrieve a set of stopwords in English to filter out common words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Stemming and removing stop words from the tokenized list\n",
    "    preprocessed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "    return preprocessed_tokens\n",
    "\n",
    "def bm25_ranking_and_generation(prompt_query, documents):\n",
    "    \"\"\"\n",
    "    Ranks documents using BM25 and generates a response using AWS Bedrock.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt_query (str): The query or prompt to search documents.\n",
    "    - documents (list): A list of documents to rank and generate responses from.\n",
    "\n",
    "    Returns:\n",
    "    - generated_response (str): The generated response from the model based on the most relevant document.\n",
    "    - doc_scores (np.ndarray): The scores of documents ranked by BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess documents using the preprocessing function\n",
    "    preprocessed_documents = [preprocess_with_huggingface(doc) for doc in documents]\n",
    "    # Initialize BM25 with preprocessed documents for relevance scoring\n",
    "    bm25 = BM25Okapi(preprocessed_documents)\n",
    "    # Preprocess the query to tokenize and stem\n",
    "    query_tokens = preprocess_with_huggingface(prompt_query)\n",
    "    # Get relevance scores of documents against the query using BM25\n",
    "    doc_scores = bm25.get_scores(query_tokens)\n",
    "    # Find the most relevant document based on the highest score\n",
    "    most_relevant_doc_index = np.argmax(doc_scores)\n",
    "    most_relevant_doc = documents[most_relevant_doc_index]\n",
    "    \n",
    "    # Define model parameters for invoking the AWS Bedrock model\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    input_key = \"prompt\"\n",
    "    output_key = \"generation\"\n",
    "    prompt_text = f\"Based on the following content, explain the role of deep learning in NLP:\\n\\n{most_relevant_doc}\\n\\nResponse:\"\n",
    "    \n",
    "    # Invoke model with the most relevant document and generate a response\n",
    "    generated_response = invoke_model(model_id, prompt_text, input_key, output_key)\n",
    "    return generated_response, doc_scores\n",
    "\n",
    "def generate_student_specific_recommendation(student_name, student_scores, textbook_data):\n",
    "    \"\"\"\n",
    "    Generates a personalized learning recommendation for a student based on their performance.\n",
    "\n",
    "    Parameters:\n",
    "    - student_name (str): The name of the student.\n",
    "    - student_scores (dict): A dictionary containing student scores by chapter.\n",
    "    - textbook_data (dict): A dictionary of textbook chapters and their content.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated personalized learning recommendation.\n",
    "    - doc_scores (np.ndarray): The scores of documents ranked by BM25.\n",
    "    \"\"\"\n",
    "    # Retrieve student's performance scores for each chapter\n",
    "    student_performance = student_scores[student_name]\n",
    "    \n",
    "    # Create a map of chapters for easy reference\n",
    "    textbook_chapters = list(textbook_data.keys())\n",
    "    chapter_map = {f\"Chapter {i+1}\": textbook_chapters[i] for i in range(len(textbook_chapters))}\n",
    "    \n",
    "    # Identify the chapters with the lowest scores for targeted improvement\n",
    "    weak_areas = sorted(student_performance, key=student_performance.get)[:2]\n",
    "    \n",
    "    # Compile focused learning path content based on weak areas\n",
    "    focus_text = \"\\n\\n\".join([f\"{chapter_map[chapter]}: {textbook_data[chapter_map[chapter]]}\" for chapter in weak_areas])\n",
    "    \n",
    "    # Formulate a query to get personalized learning suggestions\n",
    "    prompt_query = f\"Based on the student's performance, suggest a personalized learning path for the following chapters:\\n\\n{focus_text}\"\n",
    "    \n",
    "    # Get a response using BM25 ranking and model generation\n",
    "    response, doc_scores = bm25_ranking_and_generation(prompt_query, list(textbook_data.values()))\n",
    "    \n",
    "    return response, doc_scores\n",
    "\n",
    "def evaluate_metrics(generated_text, reference_texts, true_relevance, doc_scores):\n",
    "    \"\"\"\n",
    "    Evaluates metrics such as BLEU, Exact Match, Semantic Similarity, and relevance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_text (str): The text generated by the model.\n",
    "    - reference_texts (list): A list of reference texts for comparison.\n",
    "    - true_relevance (list): Ground truth relevance scores for evaluation.\n",
    "    - doc_scores (np.ndarray): Document scores from BM25 ranking.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (dict): A dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    # Compute BLEU score for the generated text against reference texts\n",
    "    bleu_score = sentence_bleu([ref.split() for ref in reference_texts], generated_text.split())\n",
    "\n",
    "    # Calculate Exact Match score to see if generated text matches any reference exactly\n",
    "    exact_match = max([1 if generated_text.strip() == ref.strip() else 0 for ref in reference_texts])\n",
    "\n",
    "    # Calculate Semantic Similarity between generated text and reference texts\n",
    "    generated_embedding = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "    reference_embeddings = semantic_model.encode(reference_texts, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(generated_embedding, reference_embeddings)\n",
    "    max_cosine_score = float(cosine_scores.max())\n",
    "\n",
    "    # Compute Relevance Metrics: Precision, Recall, F1-Score, nDCG, MAP\n",
    "    predicted_relevance = [1 if score > 0 else 0 for score in doc_scores]\n",
    "    precision = precision_score(true_relevance, predicted_relevance)\n",
    "    recall = recall_score(true_relevance, predicted_relevance)\n",
    "    f1 = f1_score(true_relevance, predicted_relevance)\n",
    "    ndcg = ndcg_score([true_relevance], [doc_scores])\n",
    "    avg_precision = average_precision_score(true_relevance, doc_scores)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, nDCG: {ndcg:.2f}, MAP: {avg_precision:.2f}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}, Exact Match: {exact_match}, Semantic Similarity: {max_cosine_score:.2f}\")\n",
    "\n",
    "    # Return metrics as a dictionary for further analysis\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ndcg\": ndcg,\n",
    "        \"map\": avg_precision,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"semantic_similarity\": max_cosine_score\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the script. Sets up textbook data, student scores,\n",
    "    and generates personalized learning recommendations.\n",
    "    \"\"\"\n",
    "    # Textbook data representing different chapters\n",
    "    textbook_data = {\n",
    "        \"Chapter 1: Introduction to NLP\": \"Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It began in the 1950s with research in machine translation...\",\n",
    "        \"Chapter 2: Fundamentals of Machine Learning\": \"Machine Learning is a branch of artificial intelligence that involves training algorithms on data to make predictions or decisions without explicit programming...\",\n",
    "        \"Chapter 3: Deep Learning in NLP\": \"Deep Learning is a subset of machine learning involving neural networks with many layers. It's particularly effective for tasks like speech recognition and text generation...\",\n",
    "        \"Chapter 4: NLP Applications\": \"NLP applications include machine translation, sentiment analysis, chatbots, and information retrieval. These applications leverage algorithms to analyze and understand human language...\",\n",
    "        \"Chapter 5: Advanced NLP Techniques\": \"Advanced NLP techniques involve transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT that have achieved state-of-the-art results on numerous NLP benchmarks...\"\n",
    "    }\n",
    "\n",
    "    # Student scores data\n",
    "    student_scores = {\n",
    "        \"Student A\": {\"Chapter 1\": 85, \"Chapter 2\": 70, \"Chapter 3\": 60, \"Chapter 4\": 90, \"Chapter 5\": 55},\n",
    "        \"Student B\": {\"Chapter 1\": 95, \"Chapter 2\": 85, \"Chapter 3\": 75, \"Chapter 4\": 80, \"Chapter 5\": 65},\n",
    "        \"Student C\": {\"Chapter 1\": 60, \"Chapter 2\": 50, \"Chapter 3\": 40, \"Chapter 4\": 55, \"Chapter 5\": 30},\n",
    "    }\n",
    "\n",
    "    # Generate recommendation for Student A\n",
    "    student_name = \"Student A\"\n",
    "    recommendation, doc_scores = generate_student_specific_recommendation(student_name, student_scores, textbook_data)\n",
    "\n",
    "    if recommendation:\n",
    "        print(f\"Personalized Learning Recommendation for {student_name}:\\n\", recommendation)\n",
    "        \n",
    "        # Example reference texts for evaluation purposes\n",
    "        reference_texts = [\n",
    "            \"Deep learning plays a crucial role in NLP by enabling the development of advanced techniques...\",\n",
    "            \"Deep learning involves neural networks with many layers and is effective for tasks like speech recognition...\"\n",
    "        ]\n",
    "        \n",
    "        # Generate relevance ground truth\n",
    "        true_relevance = [1, 1, 1, 1, 1]  # Assuming all documents are relevant in this example\n",
    "        \n",
    "        # Evaluate metrics\n",
    "        evaluate_metrics(recommendation, reference_texts, true_relevance, doc_scores)\n",
    "    else:\n",
    "        print(f\"Failed to generate a recommendation for {student_name}.\")\n",
    "\n",
    "    # Generate recommendation for Student B\n",
    "    student_name = \"Student B\"\n",
    "    recommendation, doc_scores = generate_student_specific_recommendation(student_name, student_scores, textbook_data)\n",
    "\n",
    "    if recommendation:\n",
    "        print(f\"\\nPersonalized Learning Recommendation for {student_name}:\\n\", recommendation)\n",
    "        \n",
    "        # Evaluate metrics for Student B\n",
    "        evaluate_metrics(recommendation, reference_texts, true_relevance, doc_scores)\n",
    "    else:\n",
    "        print(f\"Failed to generate a recommendation for {student_name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddae5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (1.14.0)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Using cached safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 sentence_transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3177658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ed8d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set the NLTK data path to ensure it looks in the correct directory\n",
    "nltk.data.path.append('/home/ec2-user/nltk_data')  # Update this path based on your environment\n",
    "\n",
    "# Then download the punkt tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d941a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
