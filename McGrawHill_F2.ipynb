{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87173b47",
   "metadata": {},
   "source": [
    "This code showcases advanced natural language processing (NLP) techniques to generate personalized educational recommendations based on a combination of student performance data and textbook content. It integrates various AI tools and libraries, such as AWS Bedrock, Hugging Face Transformers, NLTK, BM25 for document ranking, and Sentence Transformers for semantic similarity analysis. The script leverages Large Language Models (LLMs) to enhance the generation of educational content, as well as demonstrates several key functionalities:\n",
    "\n",
    "Text Preprocessing: The code preprocesses the input text using tokenization, stemming, and stop-word removal to prepare it for further analysis.\n",
    "\n",
    "Document Ranking with BM25: BM25, a popular ranking function, is used to rank textbook chapters or sections based on their relevance to a query derived from the student's weakest areas. This helps identify the most pertinent content for each student's learning needs.\n",
    "\n",
    "Model Invocation on AWS Bedrock: The code invokes a large language model hosted on AWS Bedrock to generate detailed explanations or recommendations based on the ranked content. This utilizes AWS's cloud infrastructure for AI and ML workloads, enabling sophisticated natural language understanding.\n",
    "\n",
    "Semantic Similarity Computation: Using Sentence Transformers, the code computes the semantic similarity between the generated text and reference texts. This step evaluates how closely the AI-generated content aligns with high-quality educational material.\n",
    "\n",
    "Performance Evaluation: The code assesses the generated outputs using various metrics, including BLEU score, Exact Match, Semantic Similarity, and relevance metrics (Precision, Recall, F1-Score, nDCG, MAP). These metrics provide a comprehensive understanding of the quality and relevance of the recommendations.\n",
    "Detailed Description of Outputs\n",
    "\n",
    "Personalized Learning Recommendations:\n",
    "The code generates customized learning recommendations for each student by analyzing their performance data to identify weaker areas (e.g., chapters with lower scores). It then formulates a query focused on these areas and searches the textbook content for the most relevant material to improve the studentâ€™s understanding. This personalized approach ensures that the guidance is tailored to each student's unique educational needs.\n",
    "Evaluation Metrics:\n",
    "\n",
    "BLEU Score:\n",
    "Measures the quality of the generated text by evaluating the n-gram overlap with reference texts. A higher BLEU score suggests better lexical similarity with the reference text.\n",
    "\n",
    "Exact Match:\n",
    "A binary score indicating whether the generated text exactly matches any of the reference texts. This is useful for tasks where precise matching is critical.\n",
    "\n",
    "Semantic Similarity:\n",
    "Computes cosine similarity between embeddings of the generated and reference texts using Sentence Transformers. A higher score indicates strong semantic alignment, even if the wording differs.\n",
    "\n",
    "Relevance Metrics:\n",
    "\n",
    "Precision: The proportion of relevant documents among the retrieved documents, indicating the accuracy of retrieval.\n",
    "\n",
    "Recall: The proportion of relevant documents successfully retrieved, reflecting the comprehensiveness of retrieval.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, providing a balanced evaluation measure.\n",
    "nDCG (Normalized Discounted Cumulative Gain): Evaluates the ranking quality of retrieved documents, accounting for the position of each relevant document.\n",
    "MAP (Mean Average Precision): Averages precision scores across all relevant documents to provide an overall assessment of retrieval effectiveness.\n",
    "\n",
    "Conclusion\n",
    "This script effectively integrates multiple advanced NLP and AI techniques to provide personalized educational content tailored to individual student needs. By leveraging powerful tools like AWS Bedrock, BM25, and Sentence Transformers, the script demonstrates a sophisticated approach to generating learning recommendations and evaluating their quality. The use of diverse evaluation metrics ensures high-quality, relevant educational guidance, making it a valuable tool for personalized education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0d3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personalized Learning Recommendation for Student A:\n",
      "  According to the passage, deep learning plays a significant role in NLP by enabling the development of advanced techniques such as transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT. These models have achieved state-of-the-art results on various NLP benchmarks, indicating the effectiveness of deep learning in NLP. Deep learning allows for the creation of complex models that can capture subtle patterns and relationships in language, leading to improved performance in tasks such as language translation, sentiment analysis, and text classification. (Source: https://www.nltk.org/book/ch06.html) ...more\n",
      "Deep learning is a subset of machine learning that involves the use of artificial neural networks to analyze and interpret data. In the context of NLP, deep learning is used to develop models that can learn to recognize and generate human language.\n",
      "\n",
      "Some of the key applications of deep learning in NLP include:\n",
      "\n",
      "1. **Language Modeling**: Deep learning models can be trained to predict the next word in a sentence, given the context of the previous words. This is known as language modeling, and it is a fundamental task in NLP.\n",
      "2. **Text Classification**: Deep learning models can be trained to classify text into different categories, such as spam vs. non-spam emails, or positive vs. negative sentiment.\n",
      "3. **Machine Translation**: Deep learning models can be trained to translate text from one language to another.\n",
      "4. **Speech Recognition**: Deep learning models can be trained to recognize spoken language and transcribe it into text.\n",
      "5. **Question Answering**: Deep learning models can be trained to answer questions based on the content of a given text.\n",
      "\n",
      "The advantages of deep learning in NLP include:\n",
      "\n",
      "1. **Improved Accuracy**: Deep learning models can achieve higher accuracy than traditional machine learning models on many NLP tasks.\n",
      "2. **Ability to Learn Complex Patterns**: Deep learning models can learn complex patterns and relationships in language, which can be difficult or impossible for traditional machine learning models to capture.\n",
      "3. **Flexibility**: Deep learning models can be trained on large datasets and can be fine-tuned for specific tasks or domains.\n",
      "4. **Scalability**: Deep learning models can be trained on large datasets and can be deployed on large-scale systems.\n",
      "\n",
      "The challenges of deep learning in NLP include:\n",
      "\n",
      "1. **Data Quality**: Deep learning models require large amounts of high-quality data to train and fine-tune.\n",
      "2. **Computational Resources**: Deep learning models require significant computational resources to train and deploy.\n",
      "3. **Interpretability**: Deep\n",
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, nDCG: 1.00, MAP: 1.00\n",
      "BLEU Score: 0.03, Exact Match: 0, Semantic Similarity: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Personalized Learning Recommendation for Student B:\n",
      "  Deep learning plays a crucial role in NLP, particularly with the development of transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT. These models have achieved state-of-the-art results on various NLP benchmarks, indicating the effectiveness of deep learning in NLP.\n",
      "\n",
      "Explanation: Deep learning has revolutionized the field of NLP by enabling the development of sophisticated models that can learn complex patterns and relationships in language. Transformers, attention mechanisms, and large-scale language models are key components of these models. Transformers are a type of neural network architecture that is particularly well-suited for sequential data like text, allowing for efficient processing of long-range dependencies. Attention mechanisms enable the model to focus on specific parts of the input data, such as words or phrases, and weight their importance. Large-scale language models, like GPT-3 and BERT, are trained on vast amounts of text data and can generate coherent and contextually relevant text.\n",
      "\n",
      "These models have achieved state-of-the-art results on numerous NLP benchmarks, including language translation, sentiment analysis, and question answering. They have also been applied to a wide range of NLP tasks, such as text classification, named entity recognition, and machine translation. The success of these models is a testament to the power of deep learning in NLP and has opened up new possibilities for natural language processing and understanding. (Source: [1])\n",
      "\n",
      "References:\n",
      "[1] \"Deep Learning for Natural Language Processing\" by Yoshua Bengio, et al. (2016)\n",
      "\n",
      "Note: The provided content is a brief summary of the role of deep learning in NLP, and the explanation is based on the information presented. For a more comprehensive understanding of the topic, it is recommended to consult the provided reference or other sources. (Source: [1]) (End of note)\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n",
      "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, nDCG: 1.00, MAP: 1.00\n",
      "BLEU Score: 0.04, Exact Match: 0, Semantic Similarity: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import boto3  # AWS SDK for Python, used to interact with AWS services like S3, Lambda, and Bedrock for model inference.\n",
    "import json  # Standard library for JSON handling, used for serializing and deserializing data in API requests and responses.\n",
    "from rank_bm25 import BM25Okapi  # BM25Okapi is a ranking function used by search engines to rank documents based on relevance.\n",
    "from transformers import AutoTokenizer  # Hugging Face's Transformers library, used for tokenizing text for NLP models.\n",
    "from nltk.stem import PorterStemmer  # NLTK's PorterStemmer, used for stemming words to their root form.\n",
    "from nltk.corpus import stopwords  # NLTK's stopwords corpus, used to filter out common stop words in English.\n",
    "import nltk  # Natural Language Toolkit, a comprehensive library for text processing and computational linguistics.\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, ndcg_score, average_precision_score  # Scikit-learn metrics for evaluating the performance of retrieval and classification tasks.\n",
    "from nltk.translate.bleu_score import sentence_bleu  # BLEU score metric from NLTK, used to evaluate the quality of text generation models.\n",
    "from sentence_transformers import SentenceTransformer, util  # Sentence Transformers for semantic similarity computation using BERT-like models.\n",
    "import numpy as np  # NumPy, a fundamental package for scientific computing with Python, used here for numerical operations like argmax.\n",
    "\n",
    "# Ensure necessary NLTK data files are downloaded for stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the Bedrock client for AWS to interact with foundation models deployed on AWS Bedrock\n",
    "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Initialize SentenceTransformer model for semantic similarity\n",
    "semantic_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def invoke_model(model_id, prompt_text, input_key, output_key):\n",
    "    \"\"\"\n",
    "    Invokes a foundation model hosted on AWS Bedrock.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id (str): The identifier of the model to invoke on AWS Bedrock.\n",
    "    - prompt_text (str): The input prompt text for the model.\n",
    "    - input_key (str): The key used for the input in the API request body.\n",
    "    - output_key (str): The key to retrieve the output from the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - output (str): The generated output from the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Making an API call to AWS Bedrock model endpoint\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                input_key: prompt_text\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # Parse the response to extract the generated output\n",
    "        result = json.loads(response['body'].read().decode('utf-8'))\n",
    "        output = result.get(output_key, None)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during model invocation\n",
    "        print(f\"Error invoking model {model_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_with_huggingface(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text using Hugging Face tokenizer and NLTK.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessed_tokens (list): A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer from Hugging Face Transformers for text tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Tokenize the input text into smaller units (tokens)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Initialize NLTK's PorterStemmer for stemming words to their root form\n",
    "    stemmer = PorterStemmer()\n",
    "    # Retrieve a set of stopwords in English to filter out common words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Stemming and removing stop words from the tokenized list\n",
    "    preprocessed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "    return preprocessed_tokens\n",
    "\n",
    "def bm25_ranking_and_generation(prompt_query, documents):\n",
    "    \"\"\"\n",
    "    Ranks documents using BM25 and generates a response using AWS Bedrock.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt_query (str): The query or prompt to search documents.\n",
    "    - documents (list): A list of documents to rank and generate responses from.\n",
    "\n",
    "    Returns:\n",
    "    - generated_response (str): The generated response from the model based on the most relevant document.\n",
    "    - doc_scores (np.ndarray): The scores of documents ranked by BM25.\n",
    "    \"\"\"\n",
    "    # Preprocess documents using the preprocessing function\n",
    "    preprocessed_documents = [preprocess_with_huggingface(doc) for doc in documents]\n",
    "    # Initialize BM25 with preprocessed documents for relevance scoring\n",
    "    bm25 = BM25Okapi(preprocessed_documents)\n",
    "    # Preprocess the query to tokenize and stem\n",
    "    query_tokens = preprocess_with_huggingface(prompt_query)\n",
    "    # Get relevance scores of documents against the query using BM25\n",
    "    doc_scores = bm25.get_scores(query_tokens)\n",
    "    # Find the most relevant document based on the highest score\n",
    "    most_relevant_doc_index = np.argmax(doc_scores)\n",
    "    most_relevant_doc = documents[most_relevant_doc_index]\n",
    "    \n",
    "    # Define model parameters for invoking the AWS Bedrock model\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    input_key = \"prompt\"\n",
    "    output_key = \"generation\"\n",
    "    prompt_text = f\"Based on the following content, explain the role of deep learning in NLP:\\n\\n{most_relevant_doc}\\n\\nResponse:\"\n",
    "    \n",
    "    # Invoke model with the most relevant document and generate a response\n",
    "    generated_response = invoke_model(model_id, prompt_text, input_key, output_key)\n",
    "    return generated_response, doc_scores\n",
    "\n",
    "def generate_student_specific_recommendation(student_name, student_scores, textbook_data):\n",
    "    \"\"\"\n",
    "    Generates a personalized learning recommendation for a student based on their performance.\n",
    "\n",
    "    Parameters:\n",
    "    - student_name (str): The name of the student.\n",
    "    - student_scores (dict): A dictionary containing student scores by chapter.\n",
    "    - textbook_data (dict): A dictionary of textbook chapters and their content.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated personalized learning recommendation.\n",
    "    - doc_scores (np.ndarray): The scores of documents ranked by BM25.\n",
    "    \"\"\"\n",
    "    # Retrieve student's performance scores for each chapter\n",
    "    student_performance = student_scores[student_name]\n",
    "    \n",
    "    # Create a map of chapters for easy reference\n",
    "    textbook_chapters = list(textbook_data.keys())\n",
    "    chapter_map = {f\"Chapter {i+1}\": textbook_chapters[i] for i in range(len(textbook_chapters))}\n",
    "    \n",
    "    # Identify the chapters with the lowest scores for targeted improvement\n",
    "    weak_areas = sorted(student_performance, key=student_performance.get)[:2]\n",
    "    \n",
    "    # Compile focused learning path content based on weak areas\n",
    "    focus_text = \"\\n\\n\".join([f\"{chapter_map[chapter]}: {textbook_data[chapter_map[chapter]]}\" for chapter in weak_areas])\n",
    "    \n",
    "    # Formulate a query to get personalized learning suggestions\n",
    "    prompt_query = f\"Based on the student's performance, suggest a personalized learning path for the following chapters:\\n\\n{focus_text}\"\n",
    "    \n",
    "    # Get a response using BM25 ranking and model generation\n",
    "    response, doc_scores = bm25_ranking_and_generation(prompt_query, list(textbook_data.values()))\n",
    "    \n",
    "    return response, doc_scores\n",
    "\n",
    "def evaluate_metrics(generated_text, reference_texts, true_relevance, doc_scores):\n",
    "    \"\"\"\n",
    "    Evaluates metrics such as BLEU, Exact Match, Semantic Similarity, and relevance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_text (str): The text generated by the model.\n",
    "    - reference_texts (list): A list of reference texts for comparison.\n",
    "    - true_relevance (list): Ground truth relevance scores for evaluation.\n",
    "    - doc_scores (np.ndarray): Document scores from BM25 ranking.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (dict): A dictionary of calculated metrics.\n",
    "    \"\"\"\n",
    "    # Compute BLEU score for the generated text against reference texts\n",
    "    bleu_score = sentence_bleu([ref.split() for ref in reference_texts], generated_text.split())\n",
    "\n",
    "    # Calculate Exact Match score to see if generated text matches any reference exactly\n",
    "    exact_match = max([1 if generated_text.strip() == ref.strip() else 0 for ref in reference_texts])\n",
    "\n",
    "    # Calculate Semantic Similarity between generated text and reference texts\n",
    "    generated_embedding = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "    reference_embeddings = semantic_model.encode(reference_texts, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(generated_embedding, reference_embeddings)\n",
    "    max_cosine_score = float(cosine_scores.max())\n",
    "\n",
    "    # Compute Relevance Metrics: Precision, Recall, F1-Score, nDCG, MAP\n",
    "    predicted_relevance = [1 if score > 0 else 0 for score in doc_scores]\n",
    "    precision = precision_score(true_relevance, predicted_relevance)\n",
    "    recall = recall_score(true_relevance, predicted_relevance)\n",
    "    f1 = f1_score(true_relevance, predicted_relevance)\n",
    "    ndcg = ndcg_score([true_relevance], [doc_scores])\n",
    "    avg_precision = average_precision_score(true_relevance, doc_scores)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, nDCG: {ndcg:.2f}, MAP: {avg_precision:.2f}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}, Exact Match: {exact_match}, Semantic Similarity: {max_cosine_score:.2f}\")\n",
    "\n",
    "    # Return metrics as a dictionary for further analysis\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"ndcg\": ndcg,\n",
    "        \"map\": avg_precision,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"semantic_similarity\": max_cosine_score\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the script. Sets up textbook data, student scores,\n",
    "    and generates personalized learning recommendations.\n",
    "    \"\"\"\n",
    "    # Textbook data representing different chapters\n",
    "    textbook_data = {\n",
    "        \"Chapter 1: Introduction to NLP\": \"Natural Language Processing (NLP) involves the interaction between computers and humans using natural language. It began in the 1950s with research in machine translation...\",\n",
    "        \"Chapter 2: Fundamentals of Machine Learning\": \"Machine Learning is a branch of artificial intelligence that involves training algorithms on data to make predictions or decisions without explicit programming...\",\n",
    "        \"Chapter 3: Deep Learning in NLP\": \"Deep Learning is a subset of machine learning involving neural networks with many layers. It's particularly effective for tasks like speech recognition and text generation...\",\n",
    "        \"Chapter 4: NLP Applications\": \"NLP applications include machine translation, sentiment analysis, chatbots, and information retrieval. These applications leverage algorithms to analyze and understand human language...\",\n",
    "        \"Chapter 5: Advanced NLP Techniques\": \"Advanced NLP techniques involve transformers, attention mechanisms, and large-scale language models like GPT-3 and BERT that have achieved state-of-the-art results on numerous NLP benchmarks...\"\n",
    "    }\n",
    "\n",
    "    # Student scores data\n",
    "    student_scores = {\n",
    "        \"Student A\": {\"Chapter 1\": 85, \"Chapter 2\": 70, \"Chapter 3\": 60, \"Chapter 4\": 90, \"Chapter 5\": 55},\n",
    "        \"Student B\": {\"Chapter 1\": 95, \"Chapter 2\": 85, \"Chapter 3\": 75, \"Chapter 4\": 80, \"Chapter 5\": 65},\n",
    "        \"Student C\": {\"Chapter 1\": 60, \"Chapter 2\": 50, \"Chapter 3\": 40, \"Chapter 4\": 55, \"Chapter 5\": 30},\n",
    "    }\n",
    "\n",
    "    # Generate recommendation for Student A\n",
    "    student_name = \"Student A\"\n",
    "    recommendation, doc_scores = generate_student_specific_recommendation(student_name, student_scores, textbook_data)\n",
    "\n",
    "    if recommendation:\n",
    "        print(f\"Personalized Learning Recommendation for {student_name}:\\n\", recommendation)\n",
    "        \n",
    "        # Example reference texts for evaluation purposes\n",
    "        reference_texts = [\n",
    "            \"Deep learning plays a crucial role in NLP by enabling the development of advanced techniques...\",\n",
    "            \"Deep learning involves neural networks with many layers and is effective for tasks like speech recognition...\"\n",
    "        ]\n",
    "        \n",
    "        # Generate relevance ground truth\n",
    "        true_relevance = [1, 1, 1, 1, 1]  # Assuming all documents are relevant in this example\n",
    "        \n",
    "        # Evaluate metrics\n",
    "        evaluate_metrics(recommendation, reference_texts, true_relevance, doc_scores)\n",
    "    else:\n",
    "        print(f\"Failed to generate a recommendation for {student_name}.\")\n",
    "\n",
    "    # Generate recommendation for Student B\n",
    "    student_name = \"Student B\"\n",
    "    recommendation, doc_scores = generate_student_specific_recommendation(student_name, student_scores, textbook_data)\n",
    "\n",
    "    if recommendation:\n",
    "        print(f\"\\nPersonalized Learning Recommendation for {student_name}:\\n\", recommendation)\n",
    "        \n",
    "        # Evaluate metrics for Student B\n",
    "        evaluate_metrics(recommendation, reference_texts, true_relevance, doc_scores)\n",
    "    else:\n",
    "        print(f\"Failed to generate a recommendation for {student_name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0912b4",
   "metadata": {},
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b85bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01b70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
