{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91141fd4",
   "metadata": {},
   "source": [
    "Hybrid RAG Explanation:\n",
    "Hybrid RAG typically refers to a system where a retrieval mechanism is used to augment a generative model (like an LLM) with additional context or information. The model then generates responses that are more informed by the retrieved data.\n",
    "How This Script Fits:\n",
    "Retrieval: The script includes a simulated retrieval step where it retrieves predefined documents based on the user's input. Although this retrieval is static and hardcoded, it serves as a placeholder for a dynamic retrieval system.\n",
    "Augmentation: The retrieved documents are combined with the user query to provide a richer context. This combined context is then fed into the LLM for response generation.\n",
    "Generation: The script uses Amazon's Titan LLM (through the Bedrock service) to generate a response based on the augmented input. This is where the LLM integration comes into play.\n",
    "Why Itâ€™s a Hybrid RAG Demo:\n",
    "Integration of LLM: The script integrates an LLM (Titan in this case) to generate responses, leveraging the context provided by the retrieved documents.\n",
    "Demonstration of Concepts: The script demonstrates the basic concepts of RAG by showing how retrieval and generation can be combined to create more informed responses. However, it lacks a fully dynamic and scalable retrieval system and doesn't include more advanced features like fine-tuning or learning from the combined data.\n",
    "Limitations:\n",
    "Static Retrieval: The retrieval mechanism is not dynamic or connected to a real knowledge base. It's a simplified version meant for demonstration purposes.\n",
    "No Learning Component: The script does not include any training or learning process that would optimize the integration between retrieval and generation, which is often a part of more sophisticated RAG systems.\n",
    "Conclusion:\n",
    "This script can be seen as a hybrid RAG demo with LLM integration, where the key concepts of retrieval and generation are illustrated. It serves as a basic example of how RAG could work but is not a full-fledged implementation. If you were to replace the static retrieval with a dynamic system and potentially add some learning or fine-tuning components, it could become a more complete RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6398af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is now running. Type 'exit' to end the chat.\n",
      "You: Best Pizza in New York\n",
      "Chatbot: \n",
      "Joe's Pizza is a famous New York pizza restaurant that has been around since 1960. It is known for its thin crust and unique flavor.\n",
      "Lombardi's is another famous New York pizza restaurant that has been around since 1905. It is known for its deep dish pizza and unique flavor.\n",
      "Di Fara Pizza is a famous New York pizza restaurant that has been around since 1964. It is known for its thin crust and unique flavor.\n",
      "Joe's Pizza, Lombardi's, and Di Fara Pizza are all famous New York pizza restaurants known for their thin\n",
      "You: How is New York as vacation\n",
      "Chatbot: \n",
      "Sorry, this model is unable to provide opinions on cities. However, it can provide information on cities. New York City is a popular vacation destination known for its iconic landmarks, world-class museums, and diverse culture. \n",
      "You: give a ice cream shop recommendation\n",
      "Chatbot: \n",
      "The model cannot find sufficient information to answer the question.\n",
      "You: best ice cream in new york\n",
      "Chatbot: \n",
      "The Big Apple is also famous for its bagels, hot dogs, and cheesecake.\n",
      "The city is home to some of the best ice cream in the world.\n",
      "Some popular spots include:\n",
      "- Lick's Ice Cream\n",
      "- Franklin Fountain\n",
      "- Franklin Fountain\n",
      "- Lick's Ice Cream\n",
      "- Franklin Fountain\n",
      "Lick's Ice Cream\n",
      "- Franklin Fountain\n",
      "- Lick's Ice Cream\n",
      "- Franklin Fountain\n",
      "\n",
      "The model cannot find sufficient information to answer the question.\n",
      "You: exit\n",
      "Chatbot session ended.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "def retrieve_documents(query):\n",
    "    \"\"\"\n",
    "    Simulates document retrieval from a knowledge base using the input query.\n",
    "    This can be replaced with actual document retrieval logic (e.g., from an S3 bucket or database).\n",
    "    \"\"\"\n",
    "    # Placeholder retrieval function\n",
    "    # In a real implementation, this would query a database, search engine, or an API to get relevant documents\n",
    "    retrieved_docs = [\n",
    "        \"New York is famous for its pizza. Some popular spots include Joe's Pizza, Lombardi's, and Di Fara Pizza.\",\n",
    "        \"New York pizza is known for its thin crust and unique flavor, often attributed to the city's water.\"\n",
    "    ]\n",
    "    return retrieved_docs\n",
    "\n",
    "def invoke_model(prompt_text):\n",
    "    \"\"\"\n",
    "    Invokes the foundation model using the Bedrock runtime service.\n",
    "    Combines the prompt text with retrieved documents for a more informed response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId='amazon.titan-text-lite-v1',\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                'inputText': prompt_text\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read().decode('utf-8'))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking model: {e}\")\n",
    "        return None\n",
    "\n",
    "def chatbot():\n",
    "    \"\"\"\n",
    "    A simple chatbot loop that interacts with the user using a foundation model and RAG.\n",
    "    \"\"\"\n",
    "    print(\"Chatbot is now running. Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot session ended.\")\n",
    "            break\n",
    "        \n",
    "        # Simulate document retrieval based on the user input\n",
    "        retrieved_docs = retrieve_documents(user_input)\n",
    "        context = \"\\n\".join(retrieved_docs)  # Combine retrieved documents into a single context string\n",
    "        \n",
    "        # Combine user input with the context\n",
    "        combined_input = f\"User query: {user_input}\\n\\nContext:\\n{context}\"\n",
    "        \n",
    "        # Invoke the foundation model with the combined input\n",
    "        result = invoke_model(combined_input)\n",
    "        \n",
    "        if result:\n",
    "            if 'results' in result and result['results']:\n",
    "                generated_text = result['results'][0]['outputText']\n",
    "                print(f\"Chatbot: {generated_text}\")\n",
    "            else:\n",
    "                print(\"Chatbot: Sorry, I couldn't generate a response.\")\n",
    "        else:\n",
    "            print(\"Chatbot: Sorry, there was an issue processing your request.\")\n",
    "\n",
    "# Start the chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ec023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
